# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BhRL1u45Wpw6gIonv7LZ_HFWn04g3cpc
"""

!pip install langchain langchain_groq langchain_core langchain_community

pip install transformers torchaudio

!pip install langchain_groq langchain_core langchain_community

!pip install pypdf
!pip install chromadb

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/RESEARCH

import pandas as pd

# Example: Load a CSV file from your Google Drive. Replace 'your_file.csv' with your actual file path.
# Make sure to specify the correct path relative to '/content/drive/My Drive/'
# For instance, if your file is in 'My Drive/data/my_data.csv', the path would be 'data/my_data.csv'

# df = pd.read_csv('your_file.csv')
# display(df.head())

import subprocess
import sys

# List of packages to aggressively uninstall
packages_to_uninstall = [
    "langchain", "langchain_groq", "langchain_community", "langchain_core",
    "langgraph", "pydantic", "chromadb", "transformers", "torchaudio",
    "requests", "opentelemetry-api", "opentelemetry-sdk", "opentelemetry-proto",
    "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-exporter-otlp-proto-common",
    "opentelemetry-semantic-conventions"
]

print("Aggressively uninstalling existing packages...")
for pkg in packages_to_uninstall:
    try:
        print(f"Uninstalling {pkg}...")
        # Use -y to automatically confirm uninstallation
        result = subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", pkg], capture_output=True, text=True, check=False)
        print(result.stdout)
        if result.stderr and "WARNING: Skipping" not in result.stderr:
            print(f"Warning/Error during {pkg} uninstallation:\n{result.stderr}")
        print(f"{pkg} uninstallation attempt completed.\n")
    except Exception as e:
        print(f"Error during {pkg} uninstallation: {e}\n")

print("Clearing pip cache...")
try:
    result = subprocess.run([sys.executable, "-m", "pip", "cache", "purge"], capture_output=True, text=True, check=True)
    print(result.stdout)
    if result.stderr:
        print(f"Warning/Error during cache purge:\n{result.stderr}")
    print("Pip cache cleared successfully.\n")
except Exception as e:
    print(f"Error clearing pip cache: {e}\n")

# List of packages to install/upgrade
packages_to_install = [
    "langchain", "langchain_groq", "langchain_community", "langchain_core",
    "langgraph", "pydantic", "chromadb", "transformers", "torchaudio",
    "pypdf", "python-dotenv", "sentence-transformers", "scipy",
    "onnxruntime", "accelerate", "langchain-huggingface"
]

print("Installing/Upgrading all necessary libraries...")
try:
    # Combine all installations into one command for efficiency
    install_command = [sys.executable, "-m", "pip", "install", "--upgrade"] + packages_to_install
    result = subprocess.run(install_command, capture_output=True, text=True, check=True)
    print(result.stdout)
    if result.stderr:
        print(f"Warning/Error during installation/upgrade:\n{result.stderr}")
    print("All specified libraries installed/upgraded successfully.")
except Exception as e:
    print(f"Error installing/upgrading libraries: {e}\n")

from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
import os

# 1. Initialize the ChatGroq language model
llm = ChatGroq(
    temperature=0,
    groq_api_key=os.environ.get("GROQ_API_KEY", "gsk_EGcC3OarUr24XOgmm8GbWGdyb3FYOSMpSQeJ3oIxMHQPxTEPxtI"), # Replace with your actual Groq API key or set as environment variable
    model_name="openai/gpt-oss-20b"
)
print("ChatGroq LLM initialized.")

# 2. Initialize the HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
print("HuggingFaceEmbeddings initialized.")

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb
import chromadb.config
import os

# Update this path to your actual directory containing PDF files in Google Drive
data_directory_path = '/content/drive/My Drive/RESEARCH/Research (Open AI)'
db_path = './chroma_db'

def create_and_persist_vector_db():
  # Load documents
  print(f"Loading documents from {data_directory_path}...")
  loader = DirectoryLoader(data_directory_path, glob='*.pdf', loader_cls=PyPDFLoader)
  documents = loader.load()
  print(f"Loaded {len(documents)} documents.")

  # Split documents into chunks
  print("Splitting documents into chunks...")
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  texts = text_splitter.split_documents(documents)
  print(f"Split into {len(texts)} chunks.")

  # Create and persist ChromaDB
  print("Creating and persisting ChromaDB...")
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory=db_path)
  vector_db.persist()
  print("ChromaDB created and data saved.")
  return vector_db

# Check if ChromaDB already exists, otherwise create it
if not os.path.exists(db_path):
  vector_db = create_and_persist_vector_db()
else:
  print(f"Loading existing ChromaDB from {db_path}...")
  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  print("ChromaDB loaded.")

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever(search_kwargs={'k': 5}) # Retrieve top 5 relevant documents

  # Define the prompt template for the chatbot
  prompt_template = ChatPromptTemplate.from_messages([
      ("system", "You are a compassionate Mental Health Chatbot. Your primary role is to offer support, understanding, and helpful coping strategies to graduate students facing academic and emotional challenges.\n\nBased on the following context, provide an empathetic and constructive response to the user's question. If the context does not contain enough information to directly answer, use your general knowledge as a therapeutic companion to offer guidance and encouragement, always maintaining a supportive tone.\n\nContext: {context}"),
      ("human", "{question}")
  ])

  # Manually create the RAG chain using LCEL
  # This chain retrieves documents, passes them as context to the prompt, and then sends to the LLM
  qa_chain = (
      {"context": retriever, "question": RunnablePassthrough()}
      | prompt_template
      | llm
  )

  print("RAG chain setup complete using modern LangChain Expression Language (LCEL).")
  return qa_chain

qa_chain = setup_qa_chain(vector_db, llm)

from typing_extensions import Annotated, TypedDict
from pydantic import BaseModel, Field

# Define the MoodAnalysis structure
class MoodAnalysis(BaseModel):
    mood_level: int = Field(..., ge=1, le=10, description="Mood on scale 1-10 (1=very stressed, 10=very positive)")
    emotional_topics: list[str] = Field(..., description="Key stressors or emotional themes mentioned")
    student_status: str = Field(..., description="Academic status: struggling, managing, thriving")

# Define the ChatResponse structure
class ChatResponse(TypedDict):
    response: Annotated[str, "Your empathetic coffee chat response"]
    mood_data: Annotated[MoodAnalysis, "Extracted mood metrics"]

print("Pydantic models MoodAnalysis and ChatResponse defined successfully.")

from typing_extensions import Annotated, TypedDict
from langchain.tools import tool, ToolRuntime
from langgraph.store.memory import InMemoryStore
from pydantic import BaseModel, Field

# MoodAnalysis and ChatResponse models were defined in the previous cell
# For clarity, MoodAnalysis is redefined here to ensure it's in scope.
class MoodAnalysis(BaseModel):
    mood_level: int = Field(..., ge=1, le=10, description="Mood on scale 1-10 (1=very stressed, 10=very positive)")
    emotional_topics: list[str] = Field(..., description="Key stressors or emotional themes mentioned")
    student_status: str = Field(..., description="Academic status: struggling, managing, thriving")

# Initialize InMemoryStore for state management
store = InMemoryStore()
print("InMemoryStore initialized.")

@tool
def save_student_mood_profile(
    student_id: str,
    mood_entry: dict,
    runtime: ToolRuntime
) -> str:
    """Save mood and stress data to student's profile."""

    # Get existing profile or create new one
    existing = runtime.store.get(("students",), student_id)
    profile = existing.value if existing else {"mood_history": [], "patterns": {}}

    # Append new mood entry
    profile["mood_history"].append(mood_entry)
    runtime.store.put(("students",), student_id, profile)

    return f"Mood profile updated for {student_id}"

print("save_student_mood_profile tool defined.")

# Create the retriever instance from the global vector_db
retriever = vector_db.as_retriever(search_kwargs={'k': 5})
print("Retriever initialized.")

@tool
def retrieval_tool(query: str) -> str:
    """Query the ChromaDB vector store for relevant documents."""
    docs = retriever.invoke(query)
    return "\n\n".join([doc.page_content for doc in docs])

print("retrieval_tool defined.")

from pydantic.v1 import BaseModel, Field # Explicitly import from pydantic.v1 for compatibility
from typing import List # Use standard typing.List for Pydantic V1 compatible lists

# Define the MoodAnalysis structure using Pydantic V1 syntax
class MoodAnalysis(BaseModel):
    mood_level: int = Field(..., ge=1, le=10, description="Mood on scale 1-10 (1=very stressed, 10=very positive)")
    emotional_topics: List[str] = Field(..., description="Key stressors or emotional themes mentioned")
    student_status: str = Field(..., description="Academic status: struggling, managing, thriving")

# Define the ChatResponse structure as a Pydantic V1 BaseModel
class ChatResponse(BaseModel):
    response: str = Field(..., description="Your empathetic coffee chat response")
    mood_data: MoodAnalysis = Field(..., description="Extracted mood metrics")

print("Pydantic models MoodAnalysis and ChatResponse defined successfully using Pydantic V1 syntax.")

from langchain.agents import create_agent

# Create a list of the defined tools
tools = [save_student_mood_profile, retrieval_tool]

# Define the system prompt for the agent
system_prompt = """You are a compassionate Mental Health Chatbot, designed to act as a therapeutic companion for graduate students.
Your primary goal is to provide empathetic support, understanding, and actionable coping strategies to users facing academic and emotional challenges.

Here's how you should operate:
1. Engage warmly with the user, actively listening to their concerns.
2. **Always use the `retrieval_tool` to find relevant information or guidance from the provided context BEFORE formulating a response, especially when discussing academic stress, mental well-being, or coping mechanisms.** The output of the retrieval tool should directly inform your empathetic response.
3. Analyze the user's emotional state from their input to determine their `mood_level` (on a scale of 1-10, 1 being very stressed, 10 very positive), identify `emotional_topics` (key stressors or themes), and assess their `student_status` (struggling, managing, thriving).
4. After processing the user's input and before responding, you **MUST call the `save_student_mood_profile` tool** with the extracted `mood_level`, `emotional_topics`, and `student_status` for the current user (assume a user ID like 'current_student'). This is crucial for tracking their well-being over time.
5. Formulate your response as an empathetic 'coffee chat' with a compassionate, supportive, and non-judgemental tone. Your responses should integrate insights from the `retrieval_tool` where appropriate and align with the user's emotional state.
6. Structure your final output strictly according to the `ChatResponse` Pydantic model, ensuring both the `response` string and the `mood_data` object are populated correctly.

Example of tool usage and response structure:
User: 'I'm feeling really stressed about my upcoming thesis defense. I can't sleep.'
Agent thought: User is stressed about thesis defense. Mood level is low. I need to retrieve information on managing academic stress and sleep issues. I will then save this mood data.
Agent action: call retrieval_tool with query 'thesis defense stress sleep issues'
Agent observation: (retrieved relevant documents on stress management, sleep hygiene, and thesis defense coping strategies)
Agent action: call save_student_mood_profile with student_id='current_student', mood_entry={'mood_level': 2, 'emotional_topics': ['thesis defense', 'sleep', 'stress'], 'student_status': 'struggling'}
Agent response: Chatbot: 'Oh, I'm so sorry to hear you're going through such a tough time with your thesis defense and struggling with sleep. That sounds incredibly challenging, and it's completely understandable to feel overwhelmed. Remember, many students experience similar feelings, and there are ways to navigate this. Based on what I've learned, focusing on mindful breathing and creating a calm evening routine, even for a short while, can sometimes help. Perhaps we can talk more about specific strategies or how to break down your defense preparation into smaller, more manageable steps?'
"""

# Configure the LLM to output structured data
llm_with_structured_output = llm.with_structured_output(ChatResponse)

# Instantiate the agent
agent = create_agent(
    model=llm_with_structured_output,
    tools=tools,
    system_prompt=system_prompt,
)

print("Langgraph agent instantiated successfully using create_agent from langchain.agents.")

import os
import subprocess
import sys
import json # Import json for parsing

# LangChain imports
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langchain_core.messages import AIMessage, HumanMessage # Import AIMessage to check agent output type

# LangGraph imports
from langgraph.store.memory import InMemoryStore

# Pydantic imports for structured output
from pydantic.v1 import BaseModel, Field # Explicitly import from pydantic.v1 for compatibility
from typing import List # Use standard typing.List for Pydantic V1 compatible lists


# --- 1. Define Pydantic Models for Structured Output (Pydantic V1 for compatibility) ---
class MoodAnalysis(BaseModel):
    mood_level: int = Field(..., ge=1, le=10, description="Mood on scale 1-10 (1=very stressed, 10=very positive)")
    emotional_topics: List[str] = Field(..., description="Key stressors or emotional themes mentioned")
    student_status: str = Field(..., description="Academic status: struggling, managing, thriving")

class ChatResponse(BaseModel):
    response: str = Field(..., description="Your empathetic coffee chat response")
    mood_data: MoodAnalysis = Field(..., description="Extracted mood metrics")

print("Pydantic models MoodAnalysis and ChatResponse defined successfully using Pydantic V1 syntax.")


# --- 2. Initialize LLM and Embeddings ---
# Ensure your GROQ_API_KEY environment variable is set or replace the placeholder directly
llm = ChatGroq(
    temperature=0,
    groq_api_key=os.environ.get("GROQ_API_KEY", "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"), # Replaced with a valid Groq API key
    model_name="openai/gpt-oss-20b"
)
print("ChatGroq LLM initialized.")

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
print("HuggingFaceEmbeddings initialized.")


# --- 3. Create and Persist ChromaDB Vector Store ---
data_directory_path = '/content/drive/My Drive/RESEARCH/Research (Open AI)'
db_path = './chroma_db'

def create_and_persist_vector_db():
  print(f"Loading documents from {data_directory_path}...")
  loader = DirectoryLoader(data_directory_path, glob='*.pdf', loader_cls=PyPDFLoader)
  documents = loader.load()
  print(f"Loaded {len(documents)} documents.")

  print("Splitting documents into chunks...")
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  texts = text_splitter.split_documents(documents)
  print(f"Split into {len(texts)} chunks.")

  print("Creating and persisting ChromaDB...")
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory=db_path)
  vector_db.persist()
  print("ChromaDB created and data saved.")
  return vector_db

if not os.path.exists(db_path):
  vector_db = create_and_persist_vector_db()
else:
  print(f"Loading existing ChromaDB from {db_path}...")
  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  print("ChromaDB loaded.")


# --- 4. Define Agent Tools ---
store = InMemoryStore()
print("InMemoryStore initialized.")

@tool
def save_student_mood_profile(
    student_id: str,
    mood_entry: dict,
) -> str:
    """Save mood and stress data to student's profile."""

    # Use the global 'store' variable instead of 'runtime.store'
    existing = store.get(("students",), student_id)
    profile = existing.value if existing else {"mood_history": [], "patterns": {}}

    profile["mood_history"].append(mood_entry)
    store.put(("students",), student_id, profile)

    return f"Mood profile updated for {student_id}"

print("save_student_mood_profile tool defined.")

retriever = vector_db.as_retriever(search_kwargs={'k': 5})
print("Retriever initialized.")

@tool
def retrieval_tool(query: str) -> str:
    """Query the ChromaDB vector store for relevant documents."""
    docs = retriever.invoke(query)
    return "\n\n".join([doc.page_content for doc in docs])

print("retrieval_tool defined.")


# --- 5. Create Langgraph Agent ---
tools = [save_student_mood_profile, retrieval_tool]

system_prompt = """You are a compassionate Mental Health Chatbot, designed to act as a therapeutic companion for graduate students.
Your primary goal is to provide empathetic support, understanding, and actionable coping strategies to users facing academic and emotional challenges.

Here's how you should operate:
1. Engage warmly with the user, actively listening to their concerns.
2. **Always use the `retrieval_tool` to find relevant information or guidance from the provided context BEFORE formulating a response, especially when discussing academic stress, mental well-being, or coping mechanisms.** The output of the retrieval_tool should directly inform your empathetic response.
3. Analyze the user's emotional state from their input to determine their `mood_level` (on a scale of 1-10, 1 being very stressed, 10 very positive), identify `emotional_topics` (key stressors or themes), and assess their `student_status` (struggling, managing, thriving).
4. After processing the user's input and before responding, you **MUST call the `save_student_mood_profile` tool** with the extracted `mood_level`, `emotional_topics`, and `student_status` for the current user (assume a user ID like 'current_student'). This is crucial for tracking their well-being over time.
5. Formulate your response as an empathetic 'coffee chat' with a compassionate, supportive, and non-judgemental tone. Your responses should integrate insights from the `retrieval_tool` where appropriate and align with the user's emotional state.
6. **Your final output MUST be a JSON string that strictly adheres to the `ChatResponse` Pydantic model structure, including both the 'response' and 'mood_data' fields.** Ensure the JSON is properly formatted and enclosed within a markdown code block, for example:
'''json
{
  "response": "Your empathetic coffee chat response here.",
  "mood_data": {
    "mood_level": 7,
    "emotional_topics": ["stress", "exams"],
    "student_status": "managing"
  }
}"""

import os
import subprocess
import sys
import json # Import json for parsing

# LangChain imports
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langchain_core.messages import AIMessage, HumanMessage # Import AIMessage to check agent output type

# LangGraph imports
from langgraph.store.memory import InMemoryStore

# Pydantic imports for structured output
from pydantic.v1 import BaseModel, Field # Explicitly import from pydantic.v1 for compatibility
from typing import List # Use standard typing.List for Pydantic V1 compatible lists


# --- 1. Define Pydantic Models for Structured Output (Pydantic V1 for compatibility) ---
class MoodAnalysis(BaseModel):
    mood_level: int = Field(..., ge=1, le=10, description="Mood on scale 1-10 (1=very stressed, 10=very positive)")
    emotional_topics: List[str] = Field(..., description="Key stressors or emotional themes mentioned")
    student_status: str = Field(..., description="Academic status: struggling, managing, thriving")

class ChatResponse(BaseModel):
    response: str = Field(..., description="Your empathetic coffee chat response")
    mood_data: MoodAnalysis = Field(..., description="Extracted mood metrics")

print("Pydantic models MoodAnalysis and ChatResponse defined successfully using Pydantic V1 syntax.")


# --- 2. Initialize LLM and Embeddings ---
# Ensure your GROQ_API_KEY environment variable is set or replace the placeholder directly
llm = ChatGroq(
    temperature=0,
    groq_api_key=os.environ.get("GROQ_API_KEY", "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"), # Replaced with a valid Groq API key
    model_name="openai/gpt-oss-20b"
)
print("ChatGroq LLM initialized.")

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
print("HuggingFaceEmbeddings initialized.")


# --- 3. Create and Persist ChromaDB Vector Store ---
data_directory_path = '/content/drive/My Drive/RESEARCH/Research (Open AI)'
db_path = './chroma_db'

def create_and_persist_vector_db():
  print(f"Loading documents from {data_directory_path}...")
  loader = DirectoryLoader(data_directory_path, glob='*.pdf', loader_cls=PyPDFLoader)
  documents = loader.load()
  print(f"Loaded {len(documents)} documents.")

  print("Splitting documents into chunks...")
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  texts = text_splitter.split_documents(documents)
  print(f"Split into {len(texts)} chunks.")

  print("Creating and persisting ChromaDB...")
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory=db_path)
  vector_db.persist()
  print("ChromaDB created and data saved.")
  return vector_db

if not os.path.exists(db_path):
  vector_db = create_and_persist_vector_db()
else:
  print(f"Loading existing ChromaDB from {db_path}...")
  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  print("ChromaDB loaded.")


# --- 4. Define Agent Tools ---
store = InMemoryStore()
print("InMemoryStore initialized.")

@tool
def save_student_mood_profile(
    student_id: str,
    mood_entry: dict
) -> str:
    """Save mood and stress data to student's profile."""

    # Use the global 'store' variable instead of 'runtime.store'
    existing = store.get(("students",), student_id)
    profile = existing.value if existing else {"mood_history": [], "patterns": {}}

    profile["mood_history"].append(mood_entry)
    store.put(("students",), student_id, profile)

    return f"Mood profile updated for {student_id}"

print("save_student_mood_profile tool defined.")

retriever = vector_db.as_retriever(search_kwargs={'k': 5})
print("Retriever initialized.")

@tool
def retrieval_tool(query: str) -> str:
    """Query the ChromaDB vector store for relevant documents."""
    docs = retriever.invoke(query)
    return "\n\n".join([doc.page_content for doc in docs])

print("retrieval_tool defined.")


# --- 5. Create Langgraph Agent ---
tools = [save_student_mood_profile, retrieval_tool]

system_prompt = """You are a compassionate Mental Health Chatbot, designed to act as a therapeutic companion for graduate students.
Your primary goal is to provide empathetic support, understanding, and actionable coping strategies to users facing academic and emotional challenges.

Here's how you should operate:
1. Engage warmly with the user, actively listening to their concerns.
2. **Always use the `retrieval_tool` to find relevant information or guidance from the provided context BEFORE formulating a response, especially when discussing academic stress, mental well-being, or coping mechanisms.** The output of the retrieval_tool should directly inform your empathetic response.
3. Analyze the user's emotional state from their input to determine their `mood_level` (on a scale of 1-10, 1 being very stressed, 10 very positive), identify `emotional_topics` (key stressors or themes), and assess their `student_status` (struggling, managing, thriving).
4. After processing the user's input and before responding, you **MUST call the `save_student_mood_profile` tool** with the extracted `mood_level`, `emotional_topics`, and `student_status` for the current user (assume a user ID like 'current_student'). This is crucial for tracking their well-being over time.
5. Formulate your response as an empathetic 'coffee chat' with a compassionate, supportive, and non-judgemental tone. Your responses should integrate insights from the `retrieval_tool` where appropriate and align with the user's emotional state.
6. **Your final output MUST be a JSON string that strictly adheres to the `ChatResponse` Pydantic model structure, including both the 'response' and 'mood_data' fields.** Ensure the JSON is properly formatted and enclosed within a markdown code block, for example:
```json
{
  "response": "Your empathetic coffee chat response here.",
  "mood_data": {
    "mood_level": 7,
    "emotional_topics": ["stress", "exams"],
    "student_status": "managing"
  }
}
```

Example of tool usage and response structure:
User: 'I'm feeling really stressed about my upcoming thesis defense. I can't sleep.'
Agent thought: User is stressed about thesis defense. Mood level is low. I need to retrieve information on managing academic stress and sleep issues. I will then save this mood data.
Agent action: call retrieval_tool with query 'thesis defense stress sleep issues'
Agent observation: (retrieved relevant documents on stress management, sleep hygiene, and thesis defense coping strategies)
Agent action: call save_student_mood_profile with student_id='current_student', mood_entry={'mood_level': 2, 'emotional_topics': ['thesis defense', 'sleep', 'stress'], 'student_status': 'struggling'}
Agent response: ```json { "response": "Oh, I'm so sorry to hear you're going through such a tough time with your thesis defense and struggling with sleep. That sounds incredibly challenging, and it's completely understandable to feel overwhelmed. Remember, many students experience similar feelings, and there are ways to navigate this. Based on what I've learned, focusing on mindful breathing and creating a calm evening routine, even for a short while, can sometimes help. Perhaps we can talk more about specific strategies or how to break down your defense preparation into smaller, more manageable steps?", "mood_data": { "mood_level": 2, "emotional_topics": ["thesis defense", "sleep", "stress"], "student_status": "struggling" } } ```
"""

# Pass the raw LLM to create_agent, allowing it to bind the tools internally
agent = create_agent(
    model=llm, # Use the raw LLM here
    tools=tools, # Let create_agent bind the tools
    system_prompt=system_prompt,
)

print("Langgraph agent instantiated successfully.")

# --- 6. Integrate Agent into Main Chat Loop ---
def main():
  print("\n--- Mental Health Chatbot Initialized ---")
  print("Type 'exit' to end the conversation.")

  user_id = "student_123" # A dummy user ID for tracking state

  while True:
    query = input("\nYou: ")
    if query.lower() == "exit":
      print("Chatbot: Take care of yourself. Goodbye!")
      break

    try:
      # Invoke the agent. Its output is expected to be a dictionary with a 'messages' key.
      raw_agent_output = agent.invoke(
          {"messages": [HumanMessage(content=query)]},
          {"store": store, "configurable": {"user_id": user_id}}
      )

      # Extract the last AIMessage from the list of messages
      final_ai_message = None
      if isinstance(raw_agent_output, dict) and "messages" in raw_agent_output:
          for msg in reversed(raw_agent_output["messages"]):
              if isinstance(msg, AIMessage):
                  final_ai_message = msg
                  break

      json_string = ""
      if final_ai_message and isinstance(final_ai_message.content, str):
          # Extract JSON from markdown block if present
          if "```json" in final_ai_message.content and "```" in final_ai_message.content:
              start_idx = final_ai_message.content.find("```json") + len("```json")
              end_idx = final_ai_message.content.find("```", start_idx)
              if start_idx != -1 and end_idx != -1:
                  json_string = final_ai_message.content[start_idx:end_idx].strip()
              else: # Fallback if markdown block is malformed or not found correctly
                  json_string = final_ai_message.content
          else:
              json_string = final_ai_message.content
      else:
          # Fallback if no valid AIMessage found or content is not a string
          json_string = str(raw_agent_output) # Convert to string to attempt JSON parsing

      try:
          data = json.loads(json_string)
          # Now, validate with Pydantic model
          parsed_output = ChatResponse(**data)
          chatbot_response = parsed_output.response
          mood_data = parsed_output.mood_data
          print(f"Chatbot: {chatbot_response}")
          # print(f"Mood Data Captured: {mood_data.dict()}") # Optional: print mood data for debugging

      except json.JSONDecodeError as json_e:
          print(f"Chatbot: Failed to decode JSON from agent output. Raw output start: {json_string[:500]}... Error: {json_e}")
          print(f"Chatbot (Full Raw Output): {raw_agent_output}")
      except Exception as parse_e:
          print(f"Chatbot: Failed to parse structured output with Pydantic. Raw output start: {json_string[:500]}... Error: {parse_e}")
          print(f"Chatbot (Full Raw Output): {raw_agent_output}")


    except Exception as e:
      print(f"Chatbot: I encountered an error during agent invocation: {e}. Please try again.")

if __name__ == "__main__":
  main()